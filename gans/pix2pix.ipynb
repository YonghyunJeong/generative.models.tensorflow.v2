{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix\n",
    "\n",
    "* `Image-to-Image Translation with Conditional Adversarial Networks`, [arXiv:1611.07004](https://arxiv.org/abs/1611.07004)\n",
    "  * Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros\n",
    "  \n",
    "* Implemented by [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) and [`eager execution`](https://www.tensorflow.org/guide/eager).\n",
    "* This code is borrowed from [TensorFlow Tutorial code](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "train_dir = 'train/pix2pix/exp1/'\n",
    "model_name = 'pix2pix'\n",
    "max_epochs = 200\n",
    "save_model_epochs = 20\n",
    "print_steps = 10\n",
    "save_images_epochs = 5\n",
    "batch_size = 1\n",
    "learning_rate_D = 2e-4\n",
    "learning_rate_G = 2e-4\n",
    "N = 400 # number of samples in train_dataset\n",
    "\n",
    "BUFFER_SIZE = N\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "You can download this dataset and similar datasets from [here](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/). As mentioned in the [paper](https://arxiv.org/abs/1611.07004) we apply random jittering and mirroring to the training dataset.\n",
    "\n",
    "* In random jittering, the image is resized to 286 x 286 and then randomly cropped to 256 x 256\n",
    "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
    "                                      cache_subdir=os.path.abspath('../datasets'),\n",
    "                                      origin='https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz', \n",
    "                                      extract=True)\n",
    "\n",
    "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_file, is_train):\n",
    "  image = tf.read_file(image_file)\n",
    "  image = tf.image.decode_jpeg(image)\n",
    "\n",
    "  w = tf.shape(image)[1]\n",
    "\n",
    "  w = w // 2\n",
    "  real_image = image[:, :w, :]\n",
    "  input_image = image[:, w:, :]\n",
    "\n",
    "  input_image = tf.cast(input_image, tf.float32)\n",
    "  real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "  if is_train:\n",
    "    # random jittering\n",
    "    \n",
    "    # resizing to 286 x 286 x 3\n",
    "    input_image = tf.image.resize_images(input_image, [286, 286], \n",
    "                                        align_corners=True, \n",
    "                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize_images(real_image, [286, 286], \n",
    "                                        align_corners=True, \n",
    "                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "    input_image, real_image = cropped_image[0], cropped_image[1]\n",
    "\n",
    "    if np.random.random() > 0.5:\n",
    "      # random mirroring\n",
    "      input_image = tf.image.flip_left_right(input_image)\n",
    "      real_image = tf.image.flip_left_right(real_image)\n",
    "  else:\n",
    "    input_image = tf.image.resize_images(input_image, size=[IMG_HEIGHT, IMG_WIDTH], \n",
    "                                         align_corners=True, method=2)\n",
    "    real_image = tf.image.resize_images(real_image, size=[IMG_HEIGHT, IMG_WIDTH], \n",
    "                                        align_corners=True, method=2)\n",
    "  \n",
    "  # normalizing the images to [-1, 1]\n",
    "  input_image = (input_image / 127.5) - 1\n",
    "  real_image = (real_image / 127.5) - 1\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.data to create batches, map(do preprocessing) and shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(PATH + 'train/*.jpg')\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.map(lambda x: load_image(x, True))\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.list_files(PATH + 'test/*.jpg')\n",
    "test_dataset = test_dataset.map(lambda x: load_image(x, False))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the generator and discriminator models\n",
    "\n",
    "* **Generator**\n",
    "  * The architecture of generator is a modified U-Net.\n",
    "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
    "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
    "  * There are skip connections between the encoder and decoder (as in U-Net).\n",
    "\n",
    "* **Discriminator**\n",
    "  * The Discriminator is a PatchGAN.\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
    "  * Discriminator receives 2 inputs.\n",
    "    * Input image and the target image, which it should classify as real.\n",
    "    * Input image and the generated image (output of generator), which it should classify as fake.\n",
    "    * We concatenate these 2 inputs together in the code (tf.concat([inp, tar], axis=-1))\n",
    "  * Shape of the input travelling through the generator and the discriminator is in the comments in the code.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(tf.keras.Model):\n",
    "  def __init__(self, filters, size, apply_batchnorm=True):\n",
    "    super(Downsample, self).__init__()\n",
    "    self.apply_batchnorm = apply_batchnorm\n",
    "    self.conv1 = layers.Conv2D(filters=filters,\n",
    "                               kernel_size=(size, size), \n",
    "                               strides=2, \n",
    "                               padding='same',\n",
    "                               kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                               use_bias=False)\n",
    "    if self.apply_batchnorm:\n",
    "      self.batchnorm = layers.BatchNormalization()\n",
    "  \n",
    "  def call(self, x, training):\n",
    "    x = self.conv1(x)\n",
    "    if self.apply_batchnorm:\n",
    "      x = self.batchnorm(x, training=training)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(tf.keras.Model):\n",
    "  def __init__(self, filters, size, apply_dropout=False):\n",
    "    super(Upsample, self).__init__()\n",
    "    self.apply_dropout = apply_dropout\n",
    "    self.up_conv = layers.Conv2DTranspose(filters=filters,\n",
    "                                          kernel_size=(size, size),\n",
    "                                          strides=2,\n",
    "                                          padding='same',\n",
    "                                          kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                                          use_bias=False)\n",
    "    self.batchnorm = layers.BatchNormalization()\n",
    "    if self.apply_dropout:\n",
    "      self.dropout = layers.Dropout(0.5)\n",
    "\n",
    "  def call(self, x1, x2, training):\n",
    "    x = self.up_conv(x1)\n",
    "    x = self.batchnorm(x, training=training)\n",
    "    if self.apply_dropout:\n",
    "      x = self.dropout(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.concat([x, x2], axis=-1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "    self.down1 = Downsample(64, 4, apply_batchnorm=False)\n",
    "    self.down2 = Downsample(128, 4)\n",
    "    self.down3 = Downsample(256, 4)\n",
    "    self.down4 = Downsample(512, 4)\n",
    "    self.down5 = Downsample(512, 4)\n",
    "    self.down6 = Downsample(512, 4)\n",
    "    self.down7 = Downsample(512, 4)\n",
    "    self.down8 = Downsample(512, 4)\n",
    "\n",
    "    self.up1 = Upsample(512, 4, apply_dropout=True)\n",
    "    self.up2 = Upsample(512, 4, apply_dropout=True)\n",
    "    self.up3 = Upsample(512, 4, apply_dropout=True)\n",
    "    self.up4 = Upsample(512, 4)\n",
    "    self.up5 = Upsample(256, 4)\n",
    "    self.up6 = Upsample(128, 4)\n",
    "    self.up7 = Upsample(64, 4)\n",
    "\n",
    "    self.last = layers.Conv2DTranspose(filters=3,\n",
    "                                       kernel_size=(4, 4),\n",
    "                                       strides=2,\n",
    "                                       padding='same',\n",
    "                                       kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "  \n",
    "  @tf.contrib.eager.defun\n",
    "  def call(self, x, training):\n",
    "    # x shape == (bs, 256, 256, 3)    \n",
    "    x1 = self.down1(x, training=training) # (bs, 128, 128, 64)\n",
    "    x2 = self.down2(x1, training=training) # (bs, 64, 64, 128)\n",
    "    x3 = self.down3(x2, training=training) # (bs, 32, 32, 256)\n",
    "    x4 = self.down4(x3, training=training) # (bs, 16, 16, 512)\n",
    "    x5 = self.down5(x4, training=training) # (bs, 8, 8, 512)\n",
    "    x6 = self.down6(x5, training=training) # (bs, 4, 4, 512)\n",
    "    x7 = self.down7(x6, training=training) # (bs, 2, 2, 512)\n",
    "    x8 = self.down8(x7, training=training) # (bs, 1, 1, 512)\n",
    "\n",
    "    x9 = self.up1(x8, x7, training=training) # (bs, 2, 2, 1024)\n",
    "    x10 = self.up2(x9, x6, training=training) # (bs, 4, 4, 1024)\n",
    "    x11 = self.up3(x10, x5, training=training) # (bs, 8, 8, 1024)\n",
    "    x12 = self.up4(x11, x4, training=training) # (bs, 16, 16, 1024)\n",
    "    x13 = self.up5(x12, x3, training=training) # (bs, 32, 32, 512)\n",
    "    x14 = self.up6(x13, x2, training=training) # (bs, 64, 64, 256)\n",
    "    x15 = self.up7(x14, x1, training=training) # (bs, 128, 128, 128)\n",
    "\n",
    "    x16 = self.last(x15) # (bs, 256, 256, 3)\n",
    "    generated_images = tf.nn.tanh(x16)\n",
    "\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscDownsample(tf.keras.Model):\n",
    "  def __init__(self, filters, size, apply_batchnorm=True):\n",
    "    super(DiscDownsample, self).__init__()\n",
    "    self.apply_batchnorm = apply_batchnorm\n",
    "    self.conv1 = layers.Conv2D(filters=filters,\n",
    "                               kernel_size=(size, size),\n",
    "                               strides=2,\n",
    "                               padding='same',\n",
    "                               kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                               use_bias=False)\n",
    "    if self.apply_batchnorm:\n",
    "      self.batchnorm = layers.BatchNormalization()\n",
    "  \n",
    "  def call(self, x, training):\n",
    "    x = self.conv1(x)\n",
    "    if self.apply_batchnorm:\n",
    "      x = self.batchnorm(x, training=training)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()    \n",
    "    self.down1 = DiscDownsample(64, 4, False)\n",
    "    self.down2 = DiscDownsample(128, 4)\n",
    "    self.down3 = DiscDownsample(256, 4)\n",
    "    \n",
    "    # we are zero padding here with 1 because we need our shape to \n",
    "    # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n",
    "    self.zero_pad1 = layers.ZeroPadding2D()\n",
    "    self.conv = layers.Conv2D(filters=512,\n",
    "                              kernel_size=(4, 4),\n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                              use_bias=False)\n",
    "    self.batchnorm1 = layers.BatchNormalization()\n",
    "    \n",
    "    # shape change from (batch_size, 31, 31, 512) to (batch_size, 30, 30, 1)\n",
    "    self.zero_pad2 = layers.ZeroPadding2D()\n",
    "    self.last = layers.Conv2D(filters=1,\n",
    "                              kernel_size=(4, 4),\n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "  \n",
    "  @tf.contrib.eager.defun\n",
    "  def call(self, inputs, targets, training):\n",
    "    # concatenating the input and the target\n",
    "    x = tf.concat([inputs, targets], axis=-1) # (bs, 256, 256, channels*2)\n",
    "    x = self.down1(x, training=training) # (bs, 128, 128, 64)\n",
    "    x = self.down2(x, training=training) # (bs, 64, 64, 128)\n",
    "    x = self.down3(x, training=training) # (bs, 32, 32, 256)\n",
    "\n",
    "    x = self.zero_pad1(x) # (bs, 34, 34, 256)\n",
    "    x = self.conv(x)      # (bs, 31, 31, 512)\n",
    "    x = self.batchnorm1(x, training=training)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    \n",
    "    x = self.zero_pad2(x) # (bs, 33, 33, 512)\n",
    "    # don't add a sigmoid activation here since\n",
    "    # the loss function expects raw logits.\n",
    "    x = self.last(x)      # (bs, 30, 30, 1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The call function of Generator and Discriminator have been decorated\n",
    "# with tf.contrib.eager.defun()\n",
    "# We get a performance speedup if defun is used (~25 seconds per epoch)\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss functions and the optimizer\n",
    "\n",
    "* **Discriminator loss**\n",
    "  * The discriminator loss function takes 2 inputs; real images, generated images\n",
    "  * real_loss is a sigmoid cross entropy loss of the real images and an array of ones(since these are the real images)\n",
    "  * generated_loss is a sigmoid cross entropy loss of the generated images and an array of zeros(since these are the fake images)\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n",
    "* **Generator loss**\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an array of ones.\n",
    "  * The paper also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GANLoss(logits, is_real=True):\n",
    "  \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "\n",
    "  Args:\n",
    "    logits (`1-rank Tensor`): logits.\n",
    "    is_real (`bool`): True means `1` labeling, False means `0` labeling.\n",
    "\n",
    "  Returns:\n",
    "    loss (`0-randk Tensor): the standard GAN loss value. (binary_cross_entropy)\n",
    "  \"\"\"\n",
    "  if is_real:\n",
    "    labels = tf.ones_like(logits)\n",
    "  else:\n",
    "    labels = tf.zeros_like(logits)\n",
    "\n",
    "  return tf.losses.sigmoid_cross_entropy(multi_class_labels=labels,\n",
    "                                         logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "#def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "  # losses of real with label \"1\"\n",
    "  real_loss = GANLoss(logits=real_logits, is_real=True)\n",
    "  # losses of fake with label \"0\"\n",
    "  fake_loss = GANLoss(logits=fake_logits, is_real=False)\n",
    "  \n",
    "  return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_logits, gen_output, target):\n",
    "  # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "  gan_loss = GANLoss(logits=fake_logits, is_real=True)\n",
    "  \n",
    "  # mean absolute error\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "  return gan_loss + (LAMBDA * l1_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator_optimizer = tf.train.RMSPropOptimizer(learning_rate_D)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate_D, beta1=0.5)\n",
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate_G, beta1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = train_dir\n",
    "if not tf.gfile.Exists(checkpoint_dir):\n",
    "  tf.gfile.MakeDirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_or_save_sample_images(test_input, target, prediction,\n",
    "                                is_save=False, epoch=None, checkpoint_dir=checkpoint_dir):\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)\n",
    "\n",
    "  display_list = [test_input[0], target[0], prediction[0]]\n",
    "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "  for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  \n",
    "  if is_save and epoch is not None:\n",
    "    filepath = os.path.join(checkpoint_dir, 'image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.savefig(filepath)\n",
    "    \n",
    "  plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the constant test input for generation (prediction) so\n",
    "# it will be easier to see the improvement of the pix2pix.\n",
    "for inputs, targets in test_dataset.take(1):\n",
    "  constant_test_input = inputs\n",
    "  constant_test_target = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info('Start Training.')\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "for epoch in range(max_epochs):\n",
    "  \n",
    "  for input_images, target in train_dataset:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(input_images, training=True)\n",
    "\n",
    "      real_logits = discriminator(input_images, target, training=True)\n",
    "      fake_logits = discriminator(input_images, generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_logits, generated_images, target)\n",
    "      disc_loss = discriminator_loss(real_logits, fake_logits)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables),\n",
    "                                        global_step=global_step)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\n",
    "        \n",
    "    epochs = global_step.numpy() * batch_size / float(N)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    if global_step.numpy() % print_steps == 0:\n",
    "      display.clear_output(wait=True)\n",
    "      examples_per_sec = batch_size / float(duration)\n",
    "      print(\"Epochs: {:.2f} global_step: {} loss_D: {:.3f} loss_G: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                epochs, global_step.numpy(), disc_loss, gen_loss, examples_per_sec, duration))\n",
    "      # generate sample image from random test image\n",
    "      # the training=True is intentional here since\n",
    "      # we want the batch statistics while running the model\n",
    "      # on the test dataset. If we use training=False, we will get \n",
    "      # the accumulated statistics learned from the training dataset\n",
    "      # (which we don't want)\n",
    "      for test_input, test_target in test_dataset.take(1):\n",
    "        prediction = generator(test_input, training=True)\n",
    "        print_or_save_sample_images(test_input, test_target, prediction)\n",
    "\n",
    "  if (epoch +1 ) % save_images_epochs == 0:\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"This images are saved at {} epoch\".format(epoch+1))\n",
    "    prediction = generator(constant_test_input, training=True)\n",
    "    print_or_save_sample_images(constant_test_input, constant_test_target, prediction,\n",
    "                                is_save=True, epoch=epoch+1, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "  # saving (checkpoint) the model every save_epochs\n",
    "  if (epoch + 1) % save_model_epochs == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating after the final epoch\n",
    "display.clear_output(wait=True)\n",
    "for test_input, target in test_dataset.take(1):\n",
    "  prediction = generator(test_input, training=True)\n",
    "  print_or_save_sample_images(test_input, target, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display an image using the epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(epoch_no, checkpoint_dir=checkpoint_dir):\n",
    "  filepath = os.path.join(checkpoint_dir, 'image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "  return PIL.Image.open(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a GIF of all the saved images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with imageio.get_writer(model_name + '.gif', mode='I') as writer:\n",
    "  filenames = glob.glob(os.path.join(checkpoint_dir, 'image*.png'))\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "    \n",
    "# this is a hack to display the gif inside the notebook\n",
    "os.system('cp {}.gif {}.gif.png'.format(model_name, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display.Image(filename=model_name + \".gif.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
